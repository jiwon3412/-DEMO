{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##신경망\n",
    "#초기화: 입력, 출력 노드 수 설정\n",
    "#학습: 데이터 통해 학습 & 가중치 업데이트\n",
    "#질의: 입력 받아 연산한 후 출력 노드에 답 전달\n",
    "#structure:\n",
    "class neuralNetwork:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def train():\n",
    "        pass\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "초기화"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 0 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-ab73fde094d4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0moutput_nodes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0mlearning_rate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mneuralNetwork\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_nodes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_nodes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_nodes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearning_rate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() takes 0 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "#입력 계층, 은닉 계층, 출력 계층의 노드 수\n",
    "#신경망의 형태와 크기를 정의 (매개 변수 통해서)\n",
    "\n",
    "def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "    #입력, 은닉, 출력 계층의 노드 개수 설정\n",
    "    self.inodes=inputnodes\n",
    "    self.hnodes=hiddennodes\n",
    "    self.onodes=outputnodes\n",
    "    #학습률\n",
    "    self.lr=learningrate\n",
    "    pass\n",
    "input_nodes=3\n",
    "hidden_nodes=3\n",
    "output_nodes=3\n",
    "learning_rate=0.3\n",
    "n=neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "노드와의 연결 노드를 생성하기\n",
    "\n",
    "가중치 - 전파 시 전달되는 신호와 역전파 시 오차 계산할 때 사용됨\n",
    "행렬로 표현\n",
    "입력 계층과 은닉 계층 사이의 가중치의 행렬 (W input_hidden)\n",
    "은닉 계층과 출력 계층 사이의 가중치의 행렬 (W hidden_output)\n",
    "처음에는 임의의 작은 값으로 초기화한다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#numpy 함수는 0과 1 사이에서 임의로 선택한 값을 원소로 가지는 행렬을 생성함\n",
    "#numpy.random,rand(rows, columns)\n",
    "import numpy\n",
    "numpy.random.rand(3, 3) -0.5\n",
    "#가중치는 원래 -1.0 ~ 1.0 사이임. 0.5를 빼줌으로써 실질적으로 -0.5 ~ 0.5가 되도록\n",
    "\n",
    "##2개의 가중치 행렬을 생성하기\n",
    "#가중치\n",
    "self.wih = (numpy.random.rand(self.hnodes, self.inodes)-0.5)\n",
    "self.who = (numpy.random.rand(self.onodes, self.hnodes)-0.5)\n",
    "\n",
    "##########더 정교한 가중치로 초기화\n",
    "self.wih=numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "self.who=numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "#0을 중심으로\n",
    "#pow(): 들어오는 노드 개수에 루트를 씌우고 역수를 취한 표준편차 (1/루트(들어오는 연결 노드 수))\n",
    "#numpy 행렬"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "질의\n",
    "query() - 입력을 받아 출력 반환\n",
    "입력 계층에서 은닉 계층을 거쳐 최종 출력 계층을 수행한다\n",
    "신호는 은닉 노드와 출력 노드로 전달될 때 가중치 연산과 활성화 함수 적용을 거친다\n",
    "\n",
    "행렬의 형태로 표현하는 것:\n",
    "가중치 연산, 신호 합, 활성화 함수 적용\n",
    "\n",
    "X hidden = W input_hidden * I\n",
    "#은닉 계층으로 들어오는 신호 = 입력 계층과 은닉 사이 가중치 행렬 * 입력 행렬"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "#은닉 계층의 각 노드로 들어오는 신호를 계산한다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "은닉 계층으로부터 나오는 신호 구하기: 시그모이드 함수 적용\n",
    "O hidden = sigmoid(X hidden)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#scipy 라이브러리 - 특수 목적 함수가 정의되어 있음\n",
    "#시그모이드 함수: expit()\n",
    "import scipy.special\n",
    "self.activation_function = lambda x: scipy.special.expit(x)\n",
    "#람다 함수: 매개변수 x를 받고 시그모이드 함수인 scipy.special.expit(x)를 반환\n",
    "#호출할 때는 self.activation_function()\n",
    "hidden_outputs= self.activation_function(hidden_inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "출력 계층"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#은닉 계층으로 들어오는 신호 계산\n",
    "hidden_inputs=numpy.dot(self.wih, inputs)\n",
    "#은닉 계층에서 나가는 신호 계산\n",
    "hidden_outputs=self.activation_function(hidden_inputs)\n",
    "#최종 출력 계층으로 들어오는 신호 계산\n",
    "final_inputs=numpy.dot(self.who, hidden_outputs)\n",
    "final_outputs=self.activation_function(final_inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "최종 신경망 클래스 코드"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.56043834],\n       [0.65520785],\n       [0.58475423]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import scipy.special\n",
    "\n",
    "class neuralNetwork:\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        self.inodes=inputnodes\n",
    "        self.hnodes=hiddennodes\n",
    "        self.onodes=outputnodes\n",
    "        #가중치 행렬: wih who\n",
    "        self.wih=numpy.random.normal(0.0, pow(self.hnodes,-0.5), (self.hnodes, self.inodes))\n",
    "        self.who=numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        #배열 내 가중치는 w_i_j로 표기. 노드 i에서 다음 계층의 노드 j로 연결됨\n",
    "        #학습률\n",
    "        self.lr=learningrate\n",
    "        #활성화 함수: 시그모이드\n",
    "        self.activation_function=lambda x: scipy.special.expit(x)\n",
    "        pass\n",
    "    def train():\n",
    "        pass\n",
    "    def query(self, inputs_list):\n",
    "        #입력 리스트를 2차원 행렬로 변환\n",
    "        inputs=numpy.array(inputs_list, ndmin=2).T\n",
    "        hidden_inputs=numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs=self.activation_function(hidden_inputs)\n",
    "        final_inputs=numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs=self.activation_function(final_inputs)\n",
    "\n",
    "        return final_outputs\n",
    "\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "learning_rate=0.3\n",
    "n=neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "n.query([1.0, 0.5, -1.5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train()\n",
    "1)출력값 계산 2) 가중치가 어떻게 업데이트되어야 하는지 알려주기 위해 오차 역전"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.61576205],\n       [0.22780455],\n       [0.81373472]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just to check the function works well\n",
    "input_nodes=3\n",
    "hidden_nodes=3\n",
    "output_nodes=3\n",
    "learning_rate=0.3\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "n.query([1.0, 0.5, -1.5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "신경망 학습\n",
    "1) 주어진 학습 데이터에 대해 결과 값을 계산 (query()함수에서 한 것처럼)\n",
    "2) 방금 계산한 결과 값을 실제의 값과 비교하고 이 차이를 이용해 가중치를 업데이트"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#1) 입력 계층의 신호를 최종 출력 계층에 전파하는 과정은 query() 함수와 동일\n",
    "#targets_list라는 매개변수가 존재한다\n",
    "def train(self, inputs_list, targets_list):\n",
    "    #입력 리스트를 2차원 행렬로 변환\n",
    "    inputs=numpy.array(inputs_list, ndmin=2).T\n",
    "    targets=numpy.array(targets_list, ndmin=2).T\n",
    "    #은닉 계층으로 들어오는 신호 계산\n",
    "    hidden_inputs=numpy.dot(self.wih, inputs)\n",
    "    hidden_outputs=self.activation_function(hidden_inputs)\n",
    "    #최종 출력 계층으로 들어오는 신호 계산\n",
    "    final_inputs=numpy.dot(self.who, hidden_outputs)\n",
    "    final_outputs=self.activation_function(final_inputs)\n",
    "\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#2) 계산 값과 실제 값 간의 오차에 기반해 가중치 업데이트\n",
    "##오차 계산 (실제 값 행렬 - 계산 값 행렬)\n",
    "#원소 간 연산 -  파이썬으로 간단하게 구현 가능\n",
    "output_errors = targets - final_outputs\n",
    "\n",
    "##은닉 계층의 노드들에 대해 역전파된 오차 계산\n",
    "#ERROR hidden = Wt hidden_output * ERROR output\n",
    "#은닉 계층의 오차는 가중치에 의해 나뉜 출력 계층의 오차들을 재조합해 계산\n",
    "hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "#은닉 계층과 최종 계층 간 가중치: output_errors 이용\n",
    "#입력 계층과 은닉 계층 간의 가중치: hidden_errors 이용\n",
    "\n",
    "#은닉 계층과 출력 계층 간의 가중치 업데이트\n",
    "self.who += self.lr * numpy.dot((output_errors*final_output* (1.0-final_outputs)), numpy.transpose(hidden_output)))\n",
    "#numpy.dot(a, b): matrix multiplication of a and b\n",
    "#transpose the last variable\n",
    "\n",
    "#입력 계층과 은닉 계층 간의 가중치 업데이트\n",
    "self.wih +=self.lr*numpy.dot((hidden_errors * hidden_outputs * (1.0-hidden_outputs)), numpy.transpose(inputs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#neural_network\n",
    "\n",
    "class neuralNetwork:\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        #matrix for weights (wih and who)\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        self.lr=learningrate\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        pass\n",
    "\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        hidden_inputs=numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs=self.activation_function(hidden_inputs)\n",
    "        final_inputs=numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs=self.activation_function(final_inputs)\n",
    "        output_errors=targets - final_outputs\n",
    "        hidden_errors=numpy.dot(self.who.T, output_errors)\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0-final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0-hidden_outputs)), numpy.transpose(input))\n",
    "        pass\n",
    "\n",
    "    def query(self, inputs_list):\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        hidden_inputs=numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs=self.activation_function(hidden_inputs)\n",
    "        final_inputs=numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs=self.activation_function(final_inputs)\n",
    "        return final_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}